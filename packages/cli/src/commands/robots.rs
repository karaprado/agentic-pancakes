use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::fs;

use crate::cli;

#[derive(Debug, Deserialize, Serialize)]
struct Policy {
    training: TrainingPolicy,
    inference: InferencePolicy,
}

#[derive(Debug, Deserialize, Serialize)]
struct TrainingPolicy {
    allowed: bool,
}

#[derive(Debug, Deserialize, Serialize)]
struct InferencePolicy {
    allowed: bool,
}

#[derive(Debug, Deserialize, Serialize)]
struct RateLimits {
    authenticated: Option<String>,
    unauthenticated: Option<String>,
}

#[derive(Debug, Deserialize, Serialize)]
struct Manifest {
    policies: Policies,
}

#[derive(Debug, Deserialize, Serialize)]
struct Policies {
    training: TrainingPolicy,
    inference: InferencePolicy,
    rate_limits: Option<RateLimits>,
}

pub async fn run(manifest_path: String, output: String) -> Result<()> {
    cli::info(&format!("Generating robots.txt from {}", manifest_path));

    // Load and parse manifest
    let manifest_content = fs::read_to_string(&manifest_path)
        .with_context(|| format!("Failed to read manifest at {}", manifest_path))?;

    let manifest: Manifest = serde_yaml::from_str(&manifest_content)
        .context("Failed to parse manifest YAML")?;

    // Generate robots.txt content
    let robots_content = generate_robots_txt(&manifest)?;

    // Write to output
    fs::write(&output, robots_content)
        .with_context(|| format!("Failed to write robots.txt to {}", output))?;

    cli::success(&format!("robots.txt generated at {}", output));

    Ok(())
}

fn generate_robots_txt(manifest: &Manifest) -> Result<String> {
    let mut output = String::new();

    // Header
    output.push_str("# robots.txt\n");
    output.push_str("# Generated by ARW CLI\n");
    output.push_str("# https://github.com/agent-ready-web/agent-ready-web\n\n");

    // Standard web crawlers
    output.push_str("# Standard Web Crawlers\n");
    output.push_str("User-agent: *\n");
    output.push_str("Allow: /\n\n");

    // AI Training Agents
    if !manifest.policies.training.allowed {
        output.push_str("# AI Training Agents - Training Not Allowed\n");
        output.push_str("# These agents are blocked from crawling for model training\n");
        output.push_str("User-agent: GPTBot\n");
        output.push_str("User-agent: ChatGPT-User\n");
        output.push_str("User-agent: Google-Extended\n");
        output.push_str("User-agent: CCBot\n");
        output.push_str("User-agent: anthropic-ai\n");
        output.push_str("User-agent: Claude-Web\n");
        output.push_str("User-agent: Omgilibot\n");
        output.push_str("User-agent: FacebookBot\n");
        output.push_str("Disallow: /\n\n");
    } else {
        output.push_str("# AI Training Agents - Training Allowed\n");
        output.push_str("User-agent: GPTBot\n");
        output.push_str("User-agent: ChatGPT-User\n");
        output.push_str("User-agent: Google-Extended\n");
        output.push_str("User-agent: CCBot\n");
        output.push_str("User-agent: anthropic-ai\n");
        output.push_str("User-agent: Claude-Web\n");
        output.push_str("Allow: /\n\n");
    }

    // AI Inference Agents
    output.push_str("# AI Inference Agents - Real-time Query Answering\n");
    output.push_str("User-agent: ChatGPT-User\n");
    output.push_str("User-agent: PerplexityBot\n");
    output.push_str("User-agent: ClaudeBot\n");
    output.push_str("User-agent: Applebot-Extended\n");
    output.push_str("User-agent: Bytespider\n");

    if manifest.policies.inference.allowed {
        output.push_str("Allow: /\n");

        // Add crawl delay if rate limit specified
        if let Some(rate_limits) = &manifest.policies.rate_limits {
            if let Some(unauth_limit) = &rate_limits.unauthenticated {
                let delay = calculate_crawl_delay(unauth_limit);
                if delay > 0 {
                    output.push_str(&format!("Crawl-delay: {}\n", delay));
                }
            }
        }
    } else {
        output.push_str("Disallow: /\n");
    }

    output.push_str("\n");

    // ARW Discovery Hints
    output.push_str("# Agent-Ready Web Discovery\n");
    output.push_str("# For ARW-compliant agents, see /llms.txt for structured discovery\n");
    output.push_str("# Specification: https://github.com/agent-ready-web/agent-ready-web\n");
    output.push_str("#\n");
    output.push_str("# ARW provides:\n");
    output.push_str("#  - Structured content discovery via /llms.txt\n");
    output.push_str("#  - Machine-readable content views (.llm.md files)\n");
    output.push_str("#  - OAuth-protected actions for transactions\n");
    output.push_str("#  - Machine-readable policies and rate limits\n");
    output.push_str("#\n");
    output.push_str("# Sitemap: /sitemap.xml\n");

    Ok(output)
}

/// Calculate crawl delay in seconds from rate limit string
/// Examples: "20/min" -> 3 seconds, "100/hour" -> 36 seconds
fn calculate_crawl_delay(rate_limit: &str) -> u32 {
    // Parse rate limit format: "N/unit"
    let parts: Vec<&str> = rate_limit.split('/').collect();
    if parts.len() != 2 {
        return 0;
    }

    let count: u32 = parts[0].parse().unwrap_or(0);
    let unit = parts[1].to_lowercase();

    if count == 0 {
        return 0;
    }

    match unit.as_str() {
        "sec" | "second" => 1,
        "min" | "minute" => 60 / count,
        "hour" => 3600 / count,
        "day" => 86400 / count,
        _ => 0,
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_calculate_crawl_delay() {
        assert_eq!(calculate_crawl_delay("20/min"), 3);
        assert_eq!(calculate_crawl_delay("100/hour"), 36);
        assert_eq!(calculate_crawl_delay("1000/day"), 86);
        assert_eq!(calculate_crawl_delay("invalid"), 0);
    }

    #[test]
    fn test_generate_robots_training_not_allowed() {
        let manifest = Manifest {
            policies: Policies {
                training: TrainingPolicy { allowed: false },
                inference: InferencePolicy { allowed: true },
                rate_limits: None,
            },
        };

        let result = generate_robots_txt(&manifest).unwrap();

        assert!(result.contains("User-agent: GPTBot"));
        assert!(result.contains("Disallow: /"));
        assert!(result.contains("AI Training Agents - Training Not Allowed"));
    }

    #[test]
    fn test_generate_robots_inference_allowed() {
        let manifest = Manifest {
            policies: Policies {
                training: TrainingPolicy { allowed: false },
                inference: InferencePolicy { allowed: true },
                rate_limits: None,
            },
        };

        let result = generate_robots_txt(&manifest).unwrap();

        assert!(result.contains("User-agent: PerplexityBot"));
        assert!(result.contains("Allow: /"));
        assert!(result.contains("Agent-Ready Web Discovery"));
    }

    #[test]
    fn test_generate_robots_with_rate_limit() {
        let manifest = Manifest {
            policies: Policies {
                training: TrainingPolicy { allowed: false },
                inference: InferencePolicy { allowed: true },
                rate_limits: Some(RateLimits {
                    authenticated: Some("100/min".to_string()),
                    unauthenticated: Some("20/min".to_string()),
                }),
            },
        };

        let result = generate_robots_txt(&manifest).unwrap();

        assert!(result.contains("Crawl-delay: 3"));
    }
}
