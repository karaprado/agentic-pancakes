# ARW CLI Standards Integration

## Complete robots.txt and sitemap.xml Support for ARW Implementation

**Version:** 1.0
**Date:** January 2025
**Related:** CLI-EXPANSION-PLAN.md

---

## Table of Contents

1. [Overview](#1-overview)
2. [robots.txt Integration](#2-robotstxt-integration)
3. [sitemap.xml Integration](#3-sitemapxml-integration)
4. [ARW-Specific Extensions](#4-arw-specific-extensions)
5. [CLI Commands](#5-cli-commands)
6. [Implementation Examples](#6-implementation-examples)
7. [Validation Rules](#7-validation-rules)

---

## 1. Overview

The ARW specification complements existing web standards rather than replacing them. This document details how the ARW CLI integrates with **robots.txt** and **sitemap.xml** standards while adding ARW-specific capabilities.

### Design Philosophy

```
┌─────────────────────────────────────────────────┐
│          Web Standards Stack                    │
├─────────────────────────────────────────────────┤
│ robots.txt      → Crawl rules and permissions   │
│ sitemap.xml     → Content index with dates      │
│ llms.txt (ARW)  → Agent capabilities manifest   │
│ *.llm.md (ARW)  → Machine-readable content      │
└─────────────────────────────────────────────────┘

Purpose Separation:
- robots.txt:  WHO can crawl WHAT
- sitemap.xml: WHERE content is, WHEN it changed
- llms.txt:    HOW agents interact, WHAT they can do
```

---

## 2. robots.txt Integration

### 2.1 Standard robots.txt Format

The ARW CLI generates standards-compliant robots.txt files following [RFC 9309](https://www.rfc-editor.org/rfc/rfc9309.html).

**Basic Structure:**

```
# Generated by ARW CLI v0.2.0
# https://github.com/nolandubeau/agent-ready-web

User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/internal/
Disallow: /.well-known/

Sitemap: https://example.com/sitemap.xml
```

### 2.2 ARW-Aware robots.txt

ARW CLI adds agent-specific rules and ARW file allowances:

```
# Generated by ARW CLI v0.2.0
# ARW-aware robots.txt
# https://github.com/nolandubeau/agent-ready-web

# General web crawlers
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/internal/
Disallow: /.well-known/
Crawl-delay: 1

# ARW discovery files (always allowed)
Allow: /llms.txt
Allow: /sitemap.llm.json
Allow: /policy.json
Allow: /*.llm.md$

# AI agent-specific rules
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: Google-Extended
User-agent: anthropic-ai
User-agent: Claude-Web
Crawl-delay: 1

# Training data crawlers (based on policy.json)
User-agent: CCBot
User-agent: anthropic-ai-training
User-agent: Google-Extended-Training
Disallow: /    # Training prohibited per ARW policy

# Standard sitemaps
Sitemap: https://example.com/sitemap.xml
Sitemap: https://example.com/sitemap.llm.json
```

### 2.3 Policy-Aware Generation

The CLI reads `policy.json` to generate appropriate robot rules:

**If `policies.training.allowed = false`:**

```
# Training prohibited
User-agent: CCBot
User-agent: anthropic-ai-training
User-agent: Google-Extended-Training
Disallow: /
```

**If `policies.training.allowed = true`:**

```
# Training allowed with restrictions
User-agent: CCBot
User-agent: anthropic-ai-training
Allow: /
Crawl-delay: 2
```

**If `policies.rate_limits` specified:**

```
# Rate limiting via crawl-delay
User-agent: *
Crawl-delay: 1  # Converted from rate_limits

User-agent: GPTBot
Crawl-delay: 0.5  # Faster for authenticated agents
```

### 2.4 AI Agent User-Agents

Common AI agent identifiers the CLI recognizes:

```yaml
# config/ai_agents.yaml (internal CLI config)
ai_agents:
  # OpenAI
  - name: GPTBot
    vendor: OpenAI
    purpose: training
  - name: ChatGPT-User
    vendor: OpenAI
    purpose: inference

  # Anthropic
  - name: anthropic-ai
    vendor: Anthropic
    purpose: both
  - name: Claude-Web
    vendor: Anthropic
    purpose: inference

  # Google
  - name: Google-Extended
    vendor: Google
    purpose: training
  - name: GoogleOther
    vendor: Google
    purpose: both

  # Perplexity
  - name: PerplexityBot
    vendor: Perplexity
    purpose: inference

  # Meta
  - name: FacebookBot
    vendor: Meta
    purpose: training

  # Apple
  - name: Applebot
    vendor: Apple
    purpose: both

  # Training-specific
  - name: CCBot
    vendor: Common Crawl
    purpose: training
  - name: omgili
    vendor: Webz.io
    purpose: training
```

### 2.5 CLI Command: `arw robots`

```bash
arw robots [SUBCOMMAND] [OPTIONS]

SUBCOMMANDS:
  generate    Generate robots.txt
  validate    Validate existing robots.txt
  test        Test if path is allowed for user-agent
  update      Update robots.txt based on policy changes

OPTIONS:
  --output <FILE>         Output file (default: robots.txt)
  --policy <FILE>         Policy file (default: policy.json)
  --allow-training        Allow AI training crawlers
  --block-training        Block AI training crawlers
  --block-paths <PATHS>   Additional paths to block (comma-separated)
  --crawl-delay <SEC>     Crawl delay in seconds (default: 1)
  --sitemap <URL>         Sitemap URL to include
  --template <NAME>       Use template: minimal, standard, strict
```

**Examples:**

```bash
# Generate standard robots.txt
arw robots generate

# Generate with training blocked
arw robots generate --block-training

# Generate with custom blocked paths
arw robots generate --block-paths /admin,/private,/internal

# Validate existing robots.txt
arw robots validate

# Test if path is allowed
arw robots test --user-agent GPTBot --path /products/keyboard.llm.md

# Update robots.txt when policy.json changes
arw robots update
```

### 2.6 Templates

**Minimal Template:**

```
User-agent: *
Allow: /

Sitemap: https://example.com/sitemap.xml
```

**Standard Template** (default):

```
User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/internal/

Allow: /llms.txt
Allow: /*.llm.md$

User-agent: GPTBot
User-agent: Claude-Web
Crawl-delay: 1

Sitemap: https://example.com/sitemap.xml
Sitemap: https://example.com/sitemap.llm.json
```

**Strict Template:**

```
User-agent: *
Disallow: /

Allow: /llms.txt
Allow: /sitemap.xml
Allow: /robots.txt

User-agent: GPTBot
User-agent: Claude-Web
Allow: /
Allow: /*.llm.md$
Crawl-delay: 2

# Training strictly prohibited
User-agent: CCBot
User-agent: anthropic-ai-training
User-agent: Google-Extended
Disallow: /

Sitemap: https://example.com/sitemap.xml
```

---

## 3. sitemap.xml Integration

### 3.1 Standard Sitemap Format

ARW CLI generates XML sitemaps following the [Sitemap Protocol](https://www.sitemaps.org/):

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://example.com/</loc>
    <lastmod>2025-01-27</lastmod>
    <changefreq>weekly</changefreq>
    <priority>1.0</priority>
  </url>
  <url>
    <loc>https://example.com/products/keyboard</loc>
    <lastmod>2025-01-25</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>
</urlset>
```

### 3.2 ARW-Enhanced Sitemap

Include machine views alongside human URLs:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:arw="https://agent-ready-web.org/schemas/sitemap/0.1">
  <!-- Human view -->
  <url>
    <loc>https://example.com/products/keyboard</loc>
    <lastmod>2025-01-25T15:30:00Z</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
    <!-- ARW extension -->
    <arw:machine-view>https://example.com/products/keyboard.llm.md</arw:machine-view>
    <arw:chunks>3</arw:chunks>
  </url>

  <!-- Machine view (separate entry) -->
  <url>
    <loc>https://example.com/products/keyboard.llm.md</loc>
    <lastmod>2025-01-25T15:30:00Z</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>

  <!-- Documentation -->
  <url>
    <loc>https://example.com/docs/api/authentication</loc>
    <lastmod>2025-01-20T10:00:00Z</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
    <arw:machine-view>https://example.com/docs/api/authentication.llm.md</arw:machine-view>
    <arw:chunks>5</arw:chunks>
  </url>

  <url>
    <loc>https://example.com/docs/api/authentication.llm.md</loc>
    <lastmod>2025-01-20T10:00:00Z</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
</urlset>
```

### 3.3 Dual Sitemap Strategy

ARW recommends generating **both** standard XML and ARW JSON sitemaps:

```
/sitemap.xml          → Standard sitemap (human + machine views)
/sitemap.llm.json     → ARW sitemap (rich metadata, chunks)
```

**Why Both?**

- **sitemap.xml** - For traditional crawlers, SEO, compatibility
- **sitemap.llm.json** - For AI agents, includes chunk metadata, topics, quality scores

### 3.4 CLI Command: `arw sitemap`

```bash
arw sitemap [OPTIONS]

OPTIONS:
  --format <FORMAT>       Format: xml, json, both (default: both)
  --output <FILE>         Output file (auto-determined by format)
  --base-url <URL>        Base URL for the site
  --source <PATH>         Source directory (default: .)
  --include <GLOB>        Include pattern (default: **/*.html,**/*.llm.md)
  --exclude <GLOB>        Exclude pattern (default: node_modules/**,dist/**)
  --priority-map <FILE>   YAML file mapping URLs to priorities
  --changefreq <FREQ>     Default change frequency (daily, weekly, monthly)
  --auto-detect           Auto-detect content types and set priorities
  --validate              Validate sitemap after generation
```

**Examples:**

```bash
# Generate both XML and JSON sitemaps
arw sitemap --base-url https://example.com

# Generate only XML
arw sitemap --format xml --base-url https://example.com

# Generate with custom priority mapping
arw sitemap --priority-map priorities.yaml

# Auto-detect and prioritize content types
arw sitemap --auto-detect
```

### 3.5 Priority Mapping

**priorities.yaml:**

```yaml
# URL pattern to priority mapping
patterns:
  - match: "^/$"
    priority: 1.0
    changefreq: weekly

  - match: "^/products/"
    priority: 0.9
    changefreq: daily

  - match: "^/docs/"
    priority: 0.8
    changefreq: weekly

  - match: "^/blog/"
    priority: 0.7
    changefreq: weekly

  - match: "\.llm\.md$"
    priority: match_parent  # Use same as parent page
    changefreq: match_parent

defaults:
  priority: 0.5
  changefreq: monthly
```

### 3.6 Sitemap Index (Large Sites)

For sites with >50,000 URLs, generate a sitemap index:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <sitemap>
    <loc>https://example.com/sitemap-products.xml</loc>
    <lastmod>2025-01-27T00:00:00Z</lastmod>
  </sitemap>
  <sitemap>
    <loc>https://example.com/sitemap-docs.xml</loc>
    <lastmod>2025-01-26T00:00:00Z</lastmod>
  </sitemap>
  <sitemap>
    <loc>https://example.com/sitemap-blog.xml</loc>
    <lastmod>2025-01-25T00:00:00Z</lastmod>
  </sitemap>
  <sitemap>
    <loc>https://example.com/sitemap-machine-views.xml</loc>
    <lastmod>2025-01-27T00:00:00Z</lastmod>
  </sitemap>
</sitemapindex>
```

**Generated with:**

```bash
arw sitemap --format xml --split-by category --max-urls 50000
```

---

## 4. ARW-Specific Extensions

### 4.1 sitemap.llm.json (ARW Format)

Rich sitemap format with AR-specific metadata:

```json
{
  "version": "0.1",
  "baseUrl": "https://example.com",
  "generatedAt": "2025-01-27T10:00:00Z",
  "totalPages": 47,
  "totalChunks": 342,
  "pages": [
    {
      "url": "/products/keyboard",
      "machineView": "/products/keyboard.llm.md",
      "priority": 0.9,
      "changeFrequency": "daily",
      "lastModified": "2025-01-25T15:30:00Z",
      "contentType": "product",
      "chunks": [
        {
          "id": "product-summary",
          "title": "Product Overview",
          "byteSize": 245,
          "tokens": 62,
          "topics": ["keyboard", "bluetooth", "price"],
          "quality": 0.95
        },
        {
          "id": "product-specs",
          "title": "Technical Specifications",
          "byteSize": 412,
          "tokens": 103,
          "topics": ["specifications", "battery", "connectivity"],
          "quality": 0.92
        }
      ],
      "actions": ["add_to_cart"],
      "languages": ["en"],
      "schema": "Product"
    },
    {
      "url": "/docs/api/authentication",
      "machineView": "/docs/api/authentication.llm.md",
      "priority": 0.8,
      "changeFrequency": "weekly",
      "lastModified": "2025-01-20T10:00:00Z",
      "contentType": "documentation",
      "chunks": [
        {
          "id": "auth-overview",
          "title": "Authentication Overview",
          "byteSize": 567,
          "tokens": 142,
          "topics": ["authentication", "oauth", "security"],
          "quality": 0.98
        },
        {
          "id": "oauth-flow",
          "title": "OAuth 2.0 Flow",
          "byteSize": 892,
          "tokens": 223,
          "topics": ["oauth", "authorization", "tokens"],
          "quality": 0.96
        }
      ],
      "actions": [],
      "languages": ["en"],
      "schema": "TechArticle"
    }
  ],
  "statistics": {
    "contentTypes": {
      "product": 12,
      "documentation": 25,
      "blog": 10
    },
    "averageChunks": 7.3,
    "averageQuality": 0.94,
    "totalTokens": 48567
  }
}
```

### 4.2 Extended XML Namespace

Custom ARW namespace for sitemap.xml:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:arw="https://agent-ready-web.org/schemas/sitemap/0.1">
  <url>
    <loc>https://example.com/products/keyboard</loc>
    <lastmod>2025-01-25T15:30:00Z</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>

    <!-- ARW extensions -->
    <arw:machine-view>https://example.com/products/keyboard.llm.md</arw:machine-view>
    <arw:chunks>3</arw:chunks>
    <arw:content-type>product</arw:content-type>
    <arw:actions>
      <arw:action id="add_to_cart"/>
    </arw:actions>
    <arw:quality>0.95</arw:quality>
    <arw:tokens>487</arw:tokens>
  </url>
</urlset>
```

---

## 5. CLI Commands

### 5.1 Complete Command Reference

#### `arw init` with Standards

```bash
arw init --standards all
# Generates: llms.txt, policy.json, robots.txt, sitemap.xml, sitemap.llm.json
```

#### `arw robots`

```bash
# Generate
arw robots generate [OPTIONS]
  --output <FILE>           Output file (default: robots.txt)
  --policy <FILE>           Policy file (default: policy.json)
  --template <NAME>         Template: minimal, standard, strict
  --allow-training          Allow AI training crawlers
  --block-training          Block AI training crawlers
  --crawl-delay <SEC>       Crawl delay in seconds

# Validate
arw robots validate [OPTIONS]
  --path <FILE>             robots.txt file to validate
  --check-arw               Check ARW-specific rules

# Test
arw robots test --user-agent <UA> --path <PATH>

# Update (based on policy.json changes)
arw robots update
```

#### `arw sitemap`

```bash
# Generate
arw sitemap [OPTIONS]
  --format <FORMAT>         xml, json, both (default: both)
  --base-url <URL>          Site base URL
  --output <FILE>           Output file
  --include <GLOB>          Include pattern
  --exclude <GLOB>          Exclude pattern
  --auto-detect             Auto-detect priorities
  --validate                Validate after generation
  --split-by <STRATEGY>     Split large sitemaps (category, size)
  --max-urls <N>            Max URLs per sitemap file

# Validate
arw sitemap validate [OPTIONS]
  --path <FILE>             Sitemap file to validate
  --format <FORMAT>         xml or json
  --check-links             Verify all URLs are accessible

# Merge
arw sitemap merge <FILE1> <FILE2> ... --output <FILE>

# Convert
arw sitemap convert --from xml --to json --input <FILE> --output <FILE>
```

#### `arw validate` with Standards Checks

```bash
arw validate --check standards
# Validates:
# - llms.txt exists and is valid
# - robots.txt allows /llms.txt
# - sitemap.xml includes all pages
# - Machine views in sitemap
# - Consistency between files
```

---

## 6. Implementation Examples

### 6.1 Full Initialization

```bash
#!/bin/bash
# Complete ARW setup with all standards

# 1. Initialize ARW
arw init \
  --template ecommerce \
  --standards all \
  --yes

# Output:
# ✓ Created .arw/config.yaml
# ✓ Created llms.txt
# ✓ Created policy.json
# ✓ Created robots.txt
# ✓ Created sitemap.xml
# ✓ Created sitemap.llm.json
# ✓ Created example.llm.md

# 2. Verify robots.txt
cat robots.txt

# 3. Verify sitemap.xml
head -n 20 sitemap.xml

# 4. Validate everything
arw validate --level comprehensive
```

### 6.2 Policy-Driven robots.txt

**Scenario:** Block AI training, allow inference

```yaml
# policy.json
{
  'version': '0.1',
  'policies':
    {
      'training': { 'allowed': false, 'note': 'Content not licensed for model training' },
      'inference': { 'allowed': true, 'restrictions': ['rate_limited', 'attribution_required'] },
      'rate_limits': { 'authenticated': '100/hour', 'unauthenticated': '20/hour' },
    },
}
```

```bash
# Generate robots.txt based on policy
arw robots generate --policy policy.json
```

**Generated robots.txt:**

```
# Generated by ARW CLI
# Based on policy.json

User-agent: *
Allow: /
Disallow: /admin/
Crawl-delay: 3  # From rate_limits.unauthenticated (20/hour → 3s)

# ARW files always allowed
Allow: /llms.txt
Allow: /*.llm.md$

# AI inference agents (allowed)
User-agent: GPTBot
User-agent: Claude-Web
User-agent: PerplexityBot
Allow: /
Crawl-delay: 1  # From rate_limits.authenticated (100/hour → 0.6s, rounded to 1s)

# Training data crawlers (blocked per policy)
User-agent: CCBot
User-agent: anthropic-ai-training
User-agent: Google-Extended
Disallow: /

Sitemap: https://example.com/sitemap.xml
Sitemap: https://example.com/sitemap.llm.json
```

### 6.3 Auto-Detect Sitemap Priorities

```bash
# Generate sitemap with auto-detected priorities
arw sitemap --auto-detect --base-url https://example.com

# Detection rules:
# - Homepage: priority 1.0, weekly
# - Products: priority 0.9, daily
# - Documentation: priority 0.8, weekly
# - Blog posts: priority 0.7, weekly
# - .llm.md files: match parent page
```

### 6.4 Large Site with Split Sitemaps

```bash
# Generate split sitemaps for large site
arw sitemap \
  --format xml \
  --base-url https://example.com \
  --split-by category \
  --max-urls 50000

# Generated files:
# - sitemap-index.xml
# - sitemap-products.xml (10,234 URLs)
# - sitemap-docs.xml (5,678 URLs)
# - sitemap-blog.xml (3,456 URLs)
# - sitemap-machine-views.xml (19,368 URLs)
```

### 6.5 CI/CD Integration

**GitHub Actions:**

```yaml
name: Update ARW Standards

on:
  push:
    branches: [main]
    paths:
      - 'content/**'
      - 'policy.json'

jobs:
  update-standards:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup ARW CLI
        uses: agent-ready-web/setup-arw@v1

      - name: Regenerate sitemaps
        run: arw sitemap --format both --base-url ${{ secrets.SITE_URL }}

      - name: Update robots.txt
        run: arw robots update

      - name: Validate
        run: arw validate --check standards

      - name: Commit changes
        run: |
          git config user.name "ARW Bot"
          git config user.email "bot@arw.dev"
          git add sitemap.xml sitemap.llm.json robots.txt
          git commit -m "chore: update sitemaps and robots.txt" || exit 0
          git push
```

---

## 7. Validation Rules

### 7.1 robots.txt Validation

```rust
// Validation checks for robots.txt

fn validate_robots_txt(content: &str, arw_config: &ArwConfig) -> ValidationResult {
    let mut issues = Vec::new();

    // 1. Check ARW files are allowed
    if !content.contains("Allow: /llms.txt") {
        issues.push(ValidationIssue::warning(
            "/llms.txt is not explicitly allowed. Add: Allow: /llms.txt"
        ));
    }

    if !content.contains("Allow: /*.llm.md") {
        issues.push(ValidationIssue::warning(
            "Machine views not explicitly allowed. Add: Allow: /*.llm.md$"
        ));
    }

    // 2. Check sitemap references
    if !content.contains("Sitemap:") {
        issues.push(ValidationIssue::error(
            "No sitemap reference found. Add: Sitemap: https://example.com/sitemap.xml"
        ));
    }

    // 3. Check policy consistency
    if arw_config.policies.allow_training == false {
        let training_agents = vec!["CCBot", "anthropic-ai-training", "Google-Extended"];
        for agent in training_agents {
            if !content.contains(&format!("User-agent: {}", agent)) {
                issues.push(ValidationIssue::warning(
                    format!("Training blocked in policy but {} not restricted in robots.txt", agent)
                ));
            }
        }
    }

    // 4. Check for common mistakes
    if content.contains("Disallow: /llms.txt") {
        issues.push(ValidationIssue::error(
            "/llms.txt is blocked! This prevents agent discovery."
        ));
    }

    ValidationResult { issues }
}
```

**Example validation output:**

```bash
$ arw robots validate

robots.txt Validation
=====================

✅ File exists: ./robots.txt
✅ Valid syntax
✅ ARW files allowed: /llms.txt, /*.llm.md
✅ Sitemap references present
✅ Policy consistent (training blocked)

⚠ Warnings (2):
  1. Crawl-delay not specified for AI agents
  2. Consider adding User-agent: PerplexityBot

Summary: PASS (2 warnings)
```

### 7.2 sitemap.xml Validation

```rust
fn validate_sitemap_xml(content: &str, arw_config: &ArwConfig) -> ValidationResult {
    let mut issues = Vec::new();

    // 1. Check XML structure
    if let Err(e) = quick_xml::de::from_str::<Urlset>(content) {
        issues.push(ValidationIssue::error(
            format!("Invalid XML: {}", e)
        ));
        return ValidationResult { issues };
    }

    // 2. Check all machine views are included
    let llms_txt = parse_llms_txt(&arw_config.site_path)?;
    for content_entry in llms_txt.content {
        if !content.contains(&content_entry.machine_view) {
            issues.push(ValidationIssue::warning(
                format!("Machine view not in sitemap: {}", content_entry.machine_view)
            ));
        }
    }

    // 3. Check lastmod dates
    let urlset: Urlset = quick_xml::de::from_str(content).unwrap();
    for url in &urlset.urls {
        if url.lastmod.is_none() {
            issues.push(ValidationIssue::info(
                format!("Missing lastmod for: {}", url.loc)
            ));
        }
    }

    // 4. Check URL accessibility
    // (Optional, can be slow for large sitemaps)
    if arw_config.validate_links {
        for url in &urlset.urls {
            if !is_url_accessible(&url.loc) {
                issues.push(ValidationIssue::error(
                    format!("URL not accessible: {}", url.loc)
                ));
            }
        }
    }

    ValidationResult { issues }
}
```

---

## Conclusion

The ARW CLI provides comprehensive integration with web standards:

### Key Features

1. **robots.txt**

   - Standards-compliant generation
   - Policy-aware AI agent rules
   - Training vs. inference differentiation
   - Automatic ARW file allowances

2. **sitemap.xml**

   - Standard XML format
   - ARW extensions for machine views
   - Dual format support (XML + JSON)
   - Large site splitting
   - Auto-detection of priorities

3. **Validation**
   - Comprehensive checks for both standards
   - Policy consistency verification
   - ARW-specific rule validation
   - Automated fix suggestions

### Best Practices

1. Always generate **both** robots.txt and sitemap.xml
2. Use `--standards all` flag during init
3. Update robots.txt when policy.json changes
4. Regenerate sitemaps after content updates
5. Validate regularly in CI/CD
6. Include machine views in sitemaps
7. Keep robots.txt policy-consistent

---

**Related Documents:**

- CLI-EXPANSION-PLAN.md
- CLI-TESTING-STRATEGY.md
- CLI-PUBLISHING-WORKFLOW.md
- ARW-v0.1-DRAFT.md (spec)
